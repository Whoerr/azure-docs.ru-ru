---
title: Устранение ошибок установки библиотек
description: В этом учебнике содержатся общие сведения об устранении ошибок при установке библиотеки.
services: synapse-analytics
author: midesa
ms.author: midesa
ms.service: synapse-analytics
ms.subservice: spark
ms.topic: conceptual
ms.date: 01/04/2021
ms.openlocfilehash: 57e9d0c584600a8fac90499d72cfac1620052603
ms.sourcegitcommit: c27a20b278f2ac758447418ea4c8c61e27927d6a
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/03/2021
ms.locfileid: "101694926"
---
# <a name="troubleshoot-library-installation-errors"></a>Устранение ошибок установки библиотек 
Чтобы сделать код, разработанный сторонним приложением, доступным для приложений, можно установить библиотеку на одном из несерверных пулов Apache Spark. Пакеты, перечисленные в файле requirements.txt, загружаются из PyPi во время запуска пула. Этот файл требований используется при каждом создании экземпляра Spark из пула Spark. После установки библиотеки для пула Spark она доступна для всех сеансов, использующих один и тот же пул. 

В некоторых случаях библиотека не появляется в пуле Apache Spark. Такая ситуация часто возникает при возникновении ошибки в предоставленных requirements.txt или указанных библиотеках. При возникновении ошибки в процессе установки библиотеки Apache Spark пул вернется к библиотекам, указанным в базовой среде выполнения синапсе.

Цель этого документа — предоставить распространенные проблемы и упростить отладку ошибок при установке библиотеки.

## <a name="force-update-your-apache-spark-pool"></a>Принудительное обновление пула Apache Spark
При обновлении библиотек в пуле Apache Spark эти изменения будут отобраны после перезапуска пула. Если у вас есть активные задания, эти задания будут по-прежнему выполняться в исходной версии пула Spark.

Можно принудительно применить изменения, выбрав параметр для **принудительного применения новых параметров**. Этот параметр приведет к завершению всех текущих сеансов для выбранного пула Spark. После завершения сеансов необходимо будет дождаться перезапуска пула. 

![Добавление библиотек Python](./media/apache-spark-azure-portal-add-libraries/update-libraries.png "Добавление библиотек Python")

## <a name="track-installation-progress"></a>Отслеживать ход установки
Зарезервированное системное задание Spark запускается каждый раз при обновлении пула с новым набором библиотек. Это задание Spark помогает отслеживать состояние установки библиотеки. Если установка завершается сбоем из-за конфликтов с библиотекой или других проблем, пул Spark вернется к предыдущему состоянию или по умолчанию. 

Кроме того, пользователи также могут проверить журналы установки, чтобы выявить конфликты зависимостей, или узнать, какие библиотеки были установлены во время обновления пула.

Для просмотра этих журналов выполните следующие действия.
1. Перейдите в список приложений Spark на вкладке " **монитор** ". 
2. Выберите Задание системного приложения Spark, соответствующее обновлению пула. Эти системные задания выполняются под названием *системресерведжоб-либрариманажемент* .
   ![Снимок экрана, на котором выделяется зарезервированное системное задание библиотеки.](./media/apache-spark-azure-portal-add-libraries/system-reserved-library-job.png "Просмотр задания системной библиотеки")
3. Переключитесь на просмотр журналов **драйверов** и **stdout** . 
4. В результатах вы увидите журналы, связанные с установкой пакетов.
    ![Снимок экрана, на котором показаны результаты задания зарезервированной системной библиотеки.](./media/apache-spark-azure-portal-add-libraries/system-reserved-library-job-results.png "Просмотр хода выполнения задания системной библиотеки")

## <a name="validate-your-permissions"></a>Проверка разрешений
Чтобы установить и обновить библиотеки, необходимо иметь разрешения на доступ к **данным BLOB-объектов хранилища** или **владельцу данных BLOB-объекта** хранилища в учетной записи хранения PRIMARY Azure Data Lake Storage 2-го поколения, связанной с рабочей областью Azure синапсе Analytics.

Чтобы проверить наличие этих разрешений, можно выполнить следующий код:

```python
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
data2 = [("James","Smith","Joe","4355","M",3000),
    ("Michael","Rose","Edward","40288","F",4000)
  ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])
 
df = spark.createDataFrame(data=data2,schema=schema)

df.write.csv("abfss://<<ENTER NAME OF FILE SYSTEM>>@<<ENTER NAME OF PRIMARY STORAGE ACCOUNT>>.dfs.core.windows.net/validate_permissions.csv")

```
Если появляется сообщение об ошибке, вероятно, отсутствуют необходимые разрешения. Сведения о том, как получить необходимые разрешения, см. в следующем документе: [назначение хранилищу данных BLOB-объекта хранилища или разрешение владельца данных BLOB-объекта хранилища](../../storage/common/storage-auth-aad-rbac-portal.md#assign-an-azure-built-in-role).

Кроме того, если вы используете конвейер, MSI рабочей области должен иметь разрешения владельца данных BLOB-объекта хранилища или права доступа к BLOB-объектам хранилища. Чтобы узнать, как предоставить удостоверению рабочей области это разрешение, перейдите [по адресу предоставление разрешений управляемому удостоверению рабочей области](../security/how-to-grant-workspace-managed-identity-permissions.md).

## <a name="check-the-environment-configuration-file"></a>Проверка файла конфигурации среды
Для обновления среды Conda можно использовать файл конфигурации среды. [Ниже](./apache-spark-manage-python-packages.md)перечислены допустимые форматы файлов для управления пулом Python.

Важно отметить следующие ограничения.
   -  Содержимое файла требований не должно содержать лишних пустых строк или символов. 
   -  [Среда выполнения синапсе](apache-spark-version-support.md) включает набор библиотек, предварительно устанавливаемых на каждый серверный пул Apache Spark. Пакеты, которые были предварительно установлены в базовую среду выполнения, не могут быть удалены.
   -  Изменение версии PySpark, Python, Scala/Java, .NET или Spark не поддерживается.
   -  Библиотеки с областью действия сеанса Python принимают файлы только с расширением YML.

## <a name="validate-wheel-files"></a>Проверка файлов колеса
Синапсе пулы бессерверных Apache Spark основаны на дистрибутиве Linux. При скачивании и установке файлов колеса непосредственно из PyPI необходимо выбрать версию, созданную на платформе Linux и работающую в той же версии Python, что и пул Spark.

>[!IMPORTANT]
>Пользовательские пакеты можно добавлять или изменять между сеансами. Однако необходимо дождаться перезапуска пула и сеанса, чтобы увидеть обновленный пакет.

## <a name="check-for-dependency-conflicts"></a>Проверка конфликтов зависимостей
 В общем случае управление разрешениями зависимости Python может оказаться непростой задачей. Чтобы упростить отладку конфликтов зависимостей, можно создать собственную виртуальную среду на основе среды выполнения синапсе и проверить изменения.

Повторное создание среды и проверка обновлений:
 1. [Скачайте](https://github.com/Azure-Samples/Synapse/blob/main/Spark/Python/base_environment.yml) шаблон, чтобы локально воссоздать среду выполнения синапсе. Между шаблоном и реальной средой синапсе могут возникнуть небольшие различия.
   
 2. Создайте виртуальную среду, выполнив [следующие инструкции](https://docs.conda.io/projects/conda/latest/user-guide/tasks/manage-environments.html). Эта среда позволяет создать изолированную установку Python с указанным списком библиотек. 
    
    ```
    conda myenv create -f environment.yml
    conda activate myenv
    ```
   
 3. Используйте ``pip install -r <provide your req.txt file>`` для обновления виртуальной среды с помощью указанных пакетов. Если установка приводит к ошибке, то может возникнуть конфликт между предварительно установленной средой выполнения синапсе и указанным в предоставленном файле требований. Эти конфликты зависимостей должны быть разрешены для получения обновленных библиотек в бессерверном пуле Apache Spark.

>[!IMPORTANT]
>Проблемы могут аррисе при использовании PIP и conda вместе. При объединении PIP и [conda рекомендуется следовать](https://docs.conda.io/projects/conda/latest/user-guide/tasks/manage-environments.html#using-pip-in-an-environment)этим рекомендациям.

## <a name="next-steps"></a>Дальнейшие действия
- Просмотр библиотек по умолчанию: [Поддержка версий Apache Spark](apache-spark-version-support.md)